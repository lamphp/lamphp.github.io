[{"title":"opencv摄像头人脸识别python","date":"2018-01-08T12:53:34.000Z","path":"2018/01/08/opencv摄像头人脸识别python/","text":"简单的opencv图片、人眼识别和摄像头人脸识别 安装12brew install opencv#Installing dependencies for opencv: sqlite, gdbm, xz, python3, gmp, mpfr, libmpc, isl, gcc, git, cmake, eigen, nasm, texi2html, lame, x264, xvid, ffmpeg, jpeg, libpng, libtiff, ilmbase, openexr, numpy, pcre, swig, tbb 图片人脸识别12345678910111213141516171819202122232425262728293031323334import numpy as npimport cv2import os#获取人脸识别训练数据,看似复杂，其实就是对于人脸特征的一些描述，这样 opencv 在读取完数据后很据训练中的样品数据，就可以感知读取到的图片上的特征，进而对图片进行人脸识别。opencv官方共享的训练数据face_cascade = cv2.CascadeClassifier(&apos;/usr/local/homebrew/Cellar/opencv/3.4.0/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml&apos;)#人眼识别训练数据eye_cascade = cv2.CascadeClassifier(&apos;/usr/local/homebrew/Cellar/opencv/3.4.0/share/OpenCV/haarcascades/haarcascade_eye.xml&apos;)#读取图片img = cv2.imread(&apos;/Users/xxx/Pictures/222.jpg&apos;)#灰度转换,灰度转换的作用就是：转换成灰度的图片的计算强度得以降低。gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)#识别人脸开始，可以随意的指定里面参数的值，来达到不同精度下的识别faces = face_cascade.detectMultiScale(gray, 1.3, 5)#scale_factor：被检测对象的尺度变化。尺度越大，越容易漏掉检测的对象，但检测速度加快；尺度越小，检测越细致准确，但检测速度变慢。#min_neighbors：数值越大，检测到对象的条件越苛刻；反之检测到对象的条件越宽松；#minSize:检测对象的大小print &quot;发现 &#123;0&#125; 个人脸!&quot;.format(len(faces))for (x,y,w,h) in faces: img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2) roi_gray = gray[y:y+h, x:x+w] roi_color = img[y:y+h, x:x+w] #识别人眼 eyes = eye_cascade.detectMultiScale(roi_gray) for (ex,ey,ew,eh) in eyes: cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)#显示图像,编辑完的图像要么直接的被显示出来，要么就保存到物理的存储介质。cv2.imshow(&apos;img&apos;,img)cv2.waitKey(0)cv2.destroyAllWindows() 摄像头人脸识别1234567891011121314151617181920212223242526import numpy as npimport cv2import oscap = cv2.VideoCapture(0) #0 打开摄像头classifier=cv2.CascadeClassifier(&quot;/usr/local/homebrew/Cellar/opencv/3.4.0/share/OpenCV/haarcascades/haarcascade_frontalface_alt.xml&quot;)print camera.isOpened() #判断摄像头是否打开while True: #读取视频流 ret,img = cap.read() #识别人脸 faceRects = classifier.detectMultiScale(img, 1.2, 2, cv2.CASCADE_SCALE_IMAGE,(20,20)) if len(faceRects)&gt;0: for faceRect in faceRects: x, y, w, h = faceRect cv2.rectangle(img, (int(x), int(y)), (int(x)+int(w), int(y)+int(h)), (0,255,0),2,0) #展示视频人脸 cv2.imshow(&apos;video&apos;,img) #q按键退出循环 key=cv2.waitKey(1) if key==ord(&apos;q&apos;): breakcap.release()cv2.destroyAllWindows()","tags":[{"name":"opencv","slug":"opencv","permalink":"http://lamphp.github.io/tags/opencv/"},{"name":"人脸识别","slug":"人脸识别","permalink":"http://lamphp.github.io/tags/人脸识别/"}]},{"title":"scrapy多spider运行","date":"2018-01-08T12:18:24.000Z","path":"2018/01/08/scrapy多spider运行/","text":"脚本运行多spider爬虫 sql123456789101112CREATE TABLE `pro_list` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT &apos;auto incrment id&apos;, `title` varchar(200) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;标题&apos;, `wangzhan` varchar(100) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;来源&apos;, `city` varchar(100) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;地区&apos;, `keyword` varchar(100) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;关键字&apos;, `url` text NOT NULL COMMENT &apos;地址&apos;, `type` int(11) NOT NULL DEFAULT &apos;0&apos;, `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &apos;创建时间&apos;, PRIMARY KEY (`id`), KEY `title` (`title`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; mysqlClient.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#coding:utf-8import MySQLdbclass PyMySQL: def __init__(self,host,user,password,charest=&apos;utf8&apos;): self.host = host self.user = user self.password = password self.charest = charest self.connect() def connect(self): try: self.conn = MySQLdb.connect(host=self.host, user=self.user, passwd=self.password,charset=self.charest) #print self.conn #查看数据库连接状态 self.cur = self.conn.cursor() #连接游标 self.conn.autocommit(True) except MySQLdb.Error,e: print &quot;Error %d: %s&quot; % (e.args[0], e.args[1]) exit(0) def selectDB(self,dbname): self.conn.select_db(dbname) #选择数据库 def query(self,sql): self.cur.execute(sql) def fetchOne(self,sql): self.query(sql) return self.cur.fetchone() def fetchAll(self,sql): self.query(sql) return self.cur.fetchall() def fetchMany(self,sql,num): self.query(sql) return self.cur.fetchmany(num) #获取所有的数据库 #return 元组 def showDatabases(self): database_sql = &apos;SHOW DATABASES&apos; return self.fetchAll(database_sql) def close(self): self.cur.close() self.conn.close() common.py1234567891011121314151617181920212223242526272829303132333435363738394041424344#coding:utf-8from mysqlClient import PyMySQLimport settings,class Common: def __init__(self): self.host = settings.MYSQL_HOST self.user = settings.MYSQL_USER self.password = settings.MYSQL_PASSWORD self.database = settings.MYSQL_DATABASE self.mysqlobj = PyMySQL(self.host, self.user, self.password) self.mysqlobj.selectDB(self.database) #过滤关键词 def matchType(self,title): result = True for key in settings.EXCLUDE_KEYWORDS: if key in title: result = False break return result #查询指定数据是否存在 def getOne(self, title): hasExists = False sql = &quot;SELECT * FROM pro_list WHERE title = &apos;&quot;+title+&quot;&apos;&quot; result = self.mysqlobj.fetchOne(sql) if result: hasExists = True return hasExists #添加到数据库 def addData(self, item): if item[&apos;is_a&apos;] == 1: type = 1 else: type = 0 all_sql = &quot;INSERT INTO pro_list (type,title,wangzhan,city,keyword,url) values (&quot;+str(type)+&quot;,&apos;&quot;+item[&apos;title&apos;]+&quot;&apos;,&apos;&quot;+item[&apos;wangzhan&apos;]+&quot;&apos;,&apos;&quot;+item[&apos;city&apos;]+&quot;&apos;,&apos;&quot;+item[&apos;keyword&apos;]+&quot;&apos;,&apos;&quot;+item[&apos;url&apos;]+&quot;&apos;)&quot; try: self.mysqlobj.query(all_sql) print &apos;insert pro_list success&apos; except: print &quot;插入失败，检查sql或者是表结构&quot; multi_spider.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100#coding:utf-8#http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/practices.html?highlight=COMMANDS#scrapy shell http://xxxx.com/import scrapyfrom scrapy.crawler import CrawlerProcessfrom twisted.internet import reactor, deferfrom scrapy.crawler import CrawlerRunnerfrom scrapy.utils.log import configure_loggingfrom scrapy.selector import Selectorfrom urllib import unquoteimport urlparse,jsonimport settingsfrom items import ProItemfrom common import Commonimport sysreload(sys)sys.setdefaultencoding(&apos;utf8&apos;)CommonObj = Common()#百度招聘class BaiduSpider(scrapy.Spider): name = &quot;baidu&quot; allowed_domains = [&quot;zhaopin.baidu.com&quot;] start_urls = [ &apos;http://zhaopin.baidu.com/search?query=&apos;, ] citys = [&apos;上海&apos;,&apos;浙江&apos;,&apos;江苏&apos;,&apos;山东&apos;,&apos;湖北&apos;,&apos;深圳&apos;,&apos;广州&apos;,&apos;厦门&apos;,&apos;珠海&apos;,&apos;福建&apos;] def parse(self, response): #简单测试页面，自动匹配下一页 i = 1 while i &lt;= 3: for keyword in settings.KEYWORDS: for city in self.citys: url = response.url+keyword+&apos;&amp;city=&apos;+city yield scrapy.Request(url, callback=self.parse_item) i+=1 def parse_item(self, response): if response.status != 200: print &quot;resquest failed&quot; return false response_url = unquote(response.url) query = urlparse.urlparse(response.url).query params = urlparse.parse_qs(query,True) keyword = params[&apos;query&apos;][0] city = params[&apos;city&apos;][0] if settings.IS_DEBUG: print &quot;baidu:&quot;+keyword + &quot;,&quot; + response_url #解析失败返回[] selector1 = Selector(response).xpath(&apos;//div[@class=&quot;jobs-item even&quot;]/dl/dt&apos;) selector2 = Selector(response).xpath(&apos;//div[@class=&quot;jobs-item odd&quot;]/dl/dt&apos;) selectors = [selector1, selector2] for sel in selectors: if sel: for sel in selector1: title = sel.xpath(&apos;span/text()&apos;).extract()[0] title = title.strip() match = CommonObj.matchType(title) if match: hasExists = CommonObj.getOne(title) if not hasExists: if settings.IS_DEBUG: print title item = ProItem() item[&apos;title&apos;] = title item[&apos;wangzhan&apos;] = &apos;baidu.com&apos; item[&apos;keyword&apos;] = keyword item[&apos;city&apos;] = city item[&apos;url&apos;] = response.url if &apos;股份有限公司&apos; in title: item[&apos;is_a&apos;] = 1 else: item[&apos;is_a&apos;] = 0 CommonObj.addData(item) else: print title + &quot;类型不匹配.&quot; else: print &quot;xpath parse failed&quot;configure_logging(&#123;&apos;LOG_FILE&apos;: settings.LOG_FILE&#125;)runner = CrawlerRunner()@defer.inlineCallbacksdef crawl(): yield runner.crawl(BaiduSpider) #yield runner.crawl(xxxSpider) #其它爬虫 reactor.stop()crawl()reactor.run()","tags":[{"name":"spider","slug":"spider","permalink":"http://lamphp.github.io/tags/spider/"},{"name":"scrapy","slug":"scrapy","permalink":"http://lamphp.github.io/tags/scrapy/"}]},{"title":"区块链入门教程【转】","date":"2017-12-29T13:24:00.000Z","path":"2017/12/29/区块链入门教程【转】/","text":"区块链（blockchain）是眼下的大热门，新闻媒体大量报道，宣称它将创造未来。可是，简单易懂的入门文章却很少。区块链到底是什么，有何特别之处，很少有解释。 【转自】http://www.ruanyifeng.com/blog/2017/12/blockchain-tutorial.html下面，我就来尝试，写一篇最好懂的区块链教程。毕竟它也不是很难的东西，核心概念非常简单，几句话就能说清楚。我希望读完本文，你不仅可以理解区块链，还会明白什么是挖矿、为什么挖矿越来越难等问题。需要说明的是，我并非这方面的专家。虽然很早就关注，但是仔细地了解区块链，还是从今年初开始。文中的错误和不准确的地方，欢迎大家指正。 一、区块链的本质 区块链是什么？一句话，它是一种特殊的分布式数据库。 首先，区块链的主要作用是储存信息。任何需要保存的信息，都可以写入区块链，也可以从里面读取，所以它是数据库。其次，任何人都可以架设服务器，加入区块链网络，成为一个节点。区块链的世界里面，没有中心节点，每个节点都是平等的，都保存着整个数据库。你可以向任何一个节点，写入/读取数据，因为所有节点最后都会同步，保证区块链一致。 二、区块链的最大特点 分布式数据库并非新发明，市场上早有此类产品。但是，区块链有一个革命性特点。区块链没有管理员，它是彻底无中心的。其他的数据库都有管理员，但是区块链没有。如果有人想对区块链添加审核，也实现不了，因为它的设计目标就是防止出现居于中心地位的管理当局。正是因为无法管理，区块链才能做到无法被控制。否则一旦大公司大集团控制了管理权，他们就会控制整个平台，其他使用者就都必须听命于他们了。但是，没有了管理员，人人都可以往里面写入数据，怎么才能保证数据是可信的呢？被坏人改了怎么办？请接着往下读，这就是区块链奇妙的地方。 三、区块 区块链由一个个区块（block）组成。区块很像数据库的记录，每次写入数据，就是创建一个区块。 每个区块包含两个部分。 区块头（Head）：记录当前区块的元信息 区块体（Body）：实际数据 区块头包含了当前区块的多项元信息。 生成时间实际数据（即区块体）的 Hash上一个区块的 Hash… 这里，你需要理解什么叫 Hash，这是理解区块链必需的。 所谓 Hash 就是计算机可以对任意内容，计算出一个长度相同的特征值。区块链的 Hash 长度是256位，这就是说，不管原始内容是什么，最后都会计算出一个256位的二进制数字。而且可以保证，只要原始内容不同，对应的 Hash 一定是不同的。 举例来说，字符串123的 Hash 是a8fdc205a9f19cc1c7507a60c4f01b13d11d7fd0（十六进制），转成二进制就是256位，而且只有123能得到这个 Hash。 因此，就有两个重要的推论。 推论1：每个区块的 Hash 都是不一样的，可以通过 Hash 标识区块。推论2：如果区块的内容变了，它的 Hash 一定会改变。 四、 Hash 的不可修改性 区块与 Hash 是一一对应的，每个区块的 Hash 都是针对”区块头”（Head）计算的。 Hash = SHA256(区块头) 上面就是区块 Hash 的计算公式，Hash 由区块头唯一决定，SHA256是区块链的 Hash 算法。 前面说过，区块头包含很多内容，其中有当前区块体的 Hash（注意是”区块体”的 Hash，而不是整个区块），还有上一个区块的 Hash。这意味着，如果当前区块的内容变了，或者上一个区块的 Hash 变了，一定会引起当前区块的 Hash 改变。 这一点对区块链有重大意义。如果有人修改了一个区块，该区块的 Hash 就变了。为了让后面的区块还能连到它，该人必须同时修改后面所有的区块，否则被改掉的区块就脱离区块链了。由于后面要提到的原因，Hash 的计算很耗时，同时修改多个区块几乎不可能发生，除非有人掌握了全网51%以上的计算能力。 正是通过这种联动机制，区块链保证了自身的可靠性，数据一旦写入，就无法被篡改。这就像历史一样，发生了就是发生了，从此再无法改变。 每个区块都连着上一个区块，这也是”区块链”这个名字的由来。 五、采矿 由于必须保证节点之间的同步，所以新区块的添加速度不能太快。试想一下，你刚刚同步了一个区块，准备基于它生成下一个区块，但这时别的节点又有新区块生成，你不得不放弃做了一半的计算，再次去同步。因为每个区块的后面，只能跟着一个区块，你永远只能在最新区块的后面，生成下一个区块。所以，你别无选择，一听到信号，就必须立刻同步。 所以，区块链的发明者中本聪（这是假名，真实身份至今未知）故意让添加新区块，变得很困难。他的设计是，平均每10分钟，全网才能生成一个新区块，一小时也就六个。 这种产出速度不是通过命令达成的，而是故意设置了海量的计算。也就是说，只有通过极其大量的计算，才能得到当前区块的有效 Hash，从而把新区块添加到区块链。由于计算量太大，所以快不起来。 这个过程就叫做采矿（mining），因为计算有效 Hash 的难度，好比在全世界的沙子里面，找到一粒符合条件的沙子。计算 Hash 的机器就叫做矿机，操作矿机的人就叫做矿工。 六、难度系数 读到这里，你可能会有一个疑问，人们都说采矿很难，可是采矿不就是用计算机算出一个 Hash 吗，这正是计算机的强项啊，怎么会变得很难，迟迟算不出来呢？ 原来不是任意一个 Hash 都可以，只有满足条件的 Hash 才会被区块链接受。这个条件特别苛刻，使得绝大部分 Hash 都不满足要求，必须重算。 原来，区块头包含一个难度系数（difficulty），这个值决定了计算 Hash 的难度。举例来说，第100000个区块的难度系数是 14484.16236122。 区块链协议规定，使用一个常量除以难度系数，可以得到目标值（target）。显然，难度系数越大，目标值就越小。 Hash 的有效性跟目标值密切相关，只有小于目标值的 Hash 才是有效的，否则 Hash 无效，必须重算。由于目标值非常小，Hash 小于该值的机会极其渺茫，可能计算10亿次，才算中一次。这就是采矿如此之慢的根本原因。 区块头里面还有一个 Nonce 值，记录了 Hash 重算的次数。第 100000 个区块的 Nonce 值是274148111，即计算了 2.74 亿次，才得到了一个有效的 Hash，该区块才能加入区块链。 七、难度系数的动态调节 就算采矿很难，但也没法保证，正好十分钟产出一个区块，有时一分钟就算出来了，有时几个小时可能也没结果。总体来看，随着硬件设备的提升，以及矿机的数量增长，计算速度一定会越来越快。 为了将产出速率恒定在十分钟，中本聪还设计了难度系数的动态调节机制。他规定，难度系数每两周（2016个区块）调整一次。如果这两周里面，区块的平均生成速度是9分钟，就意味着比法定速度快了10%，因此难度系数就要调高10%；如果平均生成速度是11分钟，就意味着比法定速度慢了10%，因此难度系数就要调低10%。 难度系数越调越高（目标值越来越小），导致了采矿越来越难。 八、区块链的分叉 即使区块链是可靠的，现在还有一个问题没有解决：如果两个人同时向区块链写入数据，也就是说，同时有两个区块加入，因为它们都连着前一个区块，就形成了分叉。这时应该采纳哪一个区块呢？ 现在的规则是，新节点总是采用最长的那条区块链。如果区块链有分叉，将看哪个分支在分叉点后面，先达到6个新区块（称为”六次确认”）。按照10分钟一个区块计算，一小时就可以确认。 由于新区块的生成速度由计算能力决定，所以这条规则就是说，拥有大多数计算能力的那条分支，就是正宗的比特链。 九、总结 区块链作为无人管理的分布式数据库，从2009年开始已经运行了8年，没有出现大的问题。这证明它是可行的。 但是，为了保证数据的可靠性，区块链也有自己的代价。一是效率，数据写入区块链，最少要等待十分钟，所有节点都同步数据，则需要更多的时间；二是能耗，区块的生成需要矿工进行无数无意义的计算，这是非常耗费能源的。 因此，区块链的适用场景，其实非常有限。 不存在所有成员都信任的管理当局 写入的数据不要求实时使用 挖矿的收益能够弥补本身的成本 如果无法满足上述的条件，那么传统的数据库是更好的解决方案。 目前，区块链最大的应用场景（可能也是唯一的应用场景），就是以比特币为代表的加密货币。下一篇文章，我将会介绍比特币的入门知识。 十、参考链接 How does blockchain really work?, by Sean HanBitcoin mining the hard way: the algorithms, protocols, and bytes, by Ken Shirriff","tags":[{"name":"区块链","slug":"区块链","permalink":"http://lamphp.github.io/tags/区块链/"}]}]